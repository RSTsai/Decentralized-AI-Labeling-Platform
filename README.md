# Decentralized-AI-Labeling-Platform
![Decentralized AI Labeling Platform Icon](https://raw.githubusercontent.com/RSTsai/Decentralized-AI-Labeling-Platform/main/Decentralized-AI-Labeling-Platform_ICON.jpg)

## Sui Overflow 2025 Hackathon
## Quick Start 
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RSTsai/Decentralized-AI-Labeling-Platform/blob/main/src/colab_env/Vision_LLM_Segmentation_Example.ipynb)

AI model Zero-shot Segmentation with SAM and CLIP 

This project demonstrates a zero-shot segmentation approach using the Segment Anything Model (SAM) and CLIP. It allows you to automatically generate masks for objects in an image and then classify these masked regions using text labels.


## Requirements

- Python 3.x
- PyTorch
- Torchvision
- OpenCV
- Matplotlib
- Segment Anything (SAM) v2.1
- CLIP

The required libraries will be automatically installed within the Colab environment.

## Setup

1.  **Open the Colab Notebook:** Open the provided Google Colab notebook.
2.  **Run All Cells:** Go to "Runtime" -> "Run all" to execute all the code cells in sequence. The notebook will handle the installation of necessary libraries and model downloads.

## Usage

1.  **Specify Image URL:** Modify the `image_url` variable in the notebook to the URL of the image you want to process.
2.  **Define Labels:** Edit the `labels_list` variable with the text labels you want to use for classifying the segmented objects.
3.  **Run the Notebook:** Execute all cells as described in the Setup section.

The notebook will perform the following steps:
- Download and resize the input image.
- Load the SAM2.1 and CLIP models.
- Generate masks for the image using SAM2.1.
- For each generated mask, crop the corresponding region from the original image.
- Use CLIP to classify the cropped image region against the provided text labels.
- Save the cropped images with filenames indicating the predicted class and confidence score.

## Configuration

- `image_url`: The URL of the input image.
- `labels_list`: A list of strings representing the potential classes for the segmented objects.
- `area_threshold_min`, `area_threshold_max`: (Currently not actively used in `show_anns_white` as shown in the code, but can be used to filter masks by area if needed.)
- `CLIP_device`, `SAM_device`: Specifies the device (GPU or CPU) for running CLIP and SAM. Colab will typically use the GPU if available.
- `sam_filename`, `model_cfg`, `sam2_checkpoint`: Configuration for the SAM2.1 model.

## Output

The processed, cropped images of the segmented objects will be saved in the same directory as the notebook, with filenames in the format `[original_filename]_[predicted_class]_[confidence_score].png`.

## Notes

- The performance of the classification depends on the quality of the masks generated by SAM and the relevance of the provided text labels.
- Adjusting SAM's automatic mask generation parameters (`points_per_side`, `pred_iou_thresh`, `stability_score_thresh`, etc.) can affect the number and quality of the generated masks.


![Decentralized AI Labeling Platform Demo](https://raw.githubusercontent.com/RSTsai/Decentralized-AI-Labeling-Platform/main/Decentralized-AI-Labeling-Platform.gif)
